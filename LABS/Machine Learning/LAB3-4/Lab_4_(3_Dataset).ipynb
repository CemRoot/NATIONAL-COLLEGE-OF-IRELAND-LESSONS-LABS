{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Task:** Fake News Detection\n",
    "\n",
    "**Ensemble:**\n",
    "\n",
    "**Features:** BoW and TF-IDF\n",
    "\n",
    "**Split:**  80:20\n",
    "\n",
    "**Base Learners:** Logistic Regression, Naive Bayes, Support Vector Machines (SVM)\n",
    "Ensemble Method: Voting Classifier (Hard or Soft voting)\n",
    "\n",
    "**Example:** Combine predictions from Logistic Regression, Naive Bayes, and SVM to classify fake news."
   ],
   "metadata": {
    "id": "3ymiKjXPZke9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Gerekli kütüphaneler\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Veri yükleme ve işleme\n",
    "fake_df = pd.read_csv('Fake News Detection Datasets/Fake.csv')\n",
    "true_df = pd.read_csv('Fake News Detection Datasets/True.csv')\n",
    "\n",
    "# Etiketleme (Fake:1, True:0)\n",
    "fake_df['label'] = 1\n",
    "true_df['label'] = 0\n",
    "\n",
    "# Veri birleştirme\n",
    "df = pd.concat([fake_df, true_df])\n",
    "\n",
    "# Metin temizleme fonksiyonu\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Küçük harfe çevirme\n",
    "    text = ''.join([c for c in text if c not in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'])  # Noktalama kaldırma\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Özellik çıkarımı (BoW + TF-IDF)\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "bow = CountVectorizer(max_features=5000)\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(df['text']).toarray()\n",
    "X_bow = bow.fit_transform(df['text']).toarray()\n",
    "\n",
    "# Özellikleri birleştirme\n",
    "X = pd.concat([pd.DataFrame(X_tfidf), pd.DataFrame(X_bow)], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Veri bölme (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Temel modeller\n",
    "lr = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "svm = SVC(probability=True)\n",
    "\n",
    "# Ensemble model (Hard Voting)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('nb', nb), ('svm', svm)],\n",
    "    voting='hard')\n",
    "\n",
    "# Model eğitimi\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Tahmin ve değerlendirme\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "print(\"Doğruluk:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nSınıflandırma Raporu:\\n\", classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "id": "rd-hr7LiZHvk",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-11T23:57:23.794762Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Task:** Blood Cell Image Classification\n",
    "\n",
    "**Ensemble:**\n",
    "\n",
    "**Features:** Convert into 1D\n",
    "\n",
    "**Split:**  80:20\n",
    "\n",
    "**Base Learners:** RF, KNN, DT\n",
    "Ensemble Method: Voting Classifier (Hard or Soft voting)\n",
    "\n",
    "**Example:** RF, KNN, DT combine to classify blood cell images."
   ],
   "metadata": {
    "id": "nWUoW2PMaDe5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def fake_news_detection():\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    from scipy.sparse import hstack\n",
    "    import string\n",
    "\n",
    "    # 1. Veri Setlerini Yükleme\n",
    "    fake_df = pd.read_csv(\"Fake News Detection Datasets/Fake.csv\")\n",
    "    true_df = pd.read_csv(\"Fake News Detection Datasets/True.csv\")\n",
    "\n",
    "    # 2. Etiketleme (Fake: 1, True: 0)\n",
    "    fake_df[\"label\"] = 1\n",
    "    true_df[\"label\"] = 0\n",
    "    df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "\n",
    "    # 3. Metin Temizleme\n",
    "    def clean_text(text):\n",
    "        text = text.lower()  # Küçük harfe çevir\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Noktalama işaretlerini kaldır\n",
    "        return text\n",
    "\n",
    "    df[\"text\"] = df[\"text\"].astype(str).apply(clean_text)\n",
    "\n",
    "    # 4. Özellik Çıkarımı (Bag-of-Words ve TF-IDF)\n",
    "    cv = CountVectorizer(max_features=5000)\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    X_cv = cv.fit_transform(df[\"text\"])\n",
    "    X_tfidf = tfidf.fit_transform(df[\"text\"])\n",
    "\n",
    "    # Her iki özellik setini yatay olarak birleştiriyoruz\n",
    "    X = hstack([X_cv, X_tfidf])\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    # 5. Eğitim ve Test Setlerine Ayırma (80:20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 6. Base Öğreniciler ve Ensemble Modeli\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    nb = MultinomialNB()\n",
    "    svc = SVC(probability=True)\n",
    "\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[(\"lr\", lr), (\"nb\", nb), (\"svc\", svc)],\n",
    "        voting=\"hard\"  # Hard Voting kullanılıyor\n",
    "    )\n",
    "\n",
    "    # 7. Model Eğitimi\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "\n",
    "    # 8. Tahmin ve Değerlendirme\n",
    "    y_pred = voting_clf.predict(X_test)\n",
    "    print(\"Doğruluk:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Sınıflandırma Raporu:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fake_news_detection()"
   ],
   "metadata": {
    "id": "l51IjjoraWGW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Task:** Audio Data Classification (Patient Health)\n",
    "\n",
    "**Ensemble:**\n",
    "\n",
    "**Features:** Use MFCC Features\n",
    "\n",
    "**Split:**  80:20\n",
    "\n",
    "**Base Learners:** RF, SVM, LR\n",
    "Ensemble Method: Voting Classifier (Hard or Soft voting)\n",
    "\n",
    "**Example:** RF, KNN, DT combine to classify blood cell images."
   ],
   "metadata": {
    "id": "0BRy9UFPaDlI"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa  # pip install librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ----------------------\n",
    "# 1) LOAD & EXTRACT MFCC\n",
    "# ----------------------\n",
    "audio_dir = \"audio_data\"  # Adjust path as needed\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for label in os.listdir(audio_dir):\n",
    "    label_path = os.path.join(audio_dir, label)\n",
    "    if os.path.isdir(label_path):\n",
    "        for file in os.listdir(label_path):\n",
    "            if file.lower().endswith(\".wav\"):\n",
    "                file_path = os.path.join(label_path, file)\n",
    "                try:\n",
    "                    signal, sr = librosa.load(file_path, sr=22050)\n",
    "                    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13)\n",
    "                    # Take mean across time axis\n",
    "                    mfcc_mean = np.mean(mfcc, axis=1)\n",
    "                    X.append(mfcc_mean)\n",
    "                    y.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not process {file_path}: {e}\")\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# ----------------------\n",
    "# 2) TRAIN-TEST SPLIT (80:20)\n",
    "# ----------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 3) DEFINE BASE LEARNERS\n",
    "# ----------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# ----------------------\n",
    "# 4) VOTING CLASSIFIER\n",
    "# ----------------------\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(\"rf\", rf), (\"knn\", knn), (\"dt\", dt)],\n",
    "    voting='hard'  # or 'soft'\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 5) TRAIN & EVALUATE\n",
    "# ----------------------\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "print(\"=== Audio Data Classification (Patient Health) ===\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ]
}
